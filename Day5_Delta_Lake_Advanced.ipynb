{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Incremental MERGE (Upserts)\n",
        "\n",
        "MERGE helps handle incremental updates by:\n",
        "- Updating matching records\n",
        "- Inserting new records\n",
        "This prevents duplicate data and avoids full overwrites."
      ],
      "metadata": {
        "id": "s47Jcr3lhd2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozufjvFjhAlt"
      },
      "outputs": [],
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "# MERGE for incremental updates\n",
        "deltaTable = DeltaTable.forPath(spark, \"/delta/events\")\n",
        "updates = spark.read.csv(\"/path/to/new_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "deltaTable.alias(\"t\").merge(\n",
        "    updates.alias(\"s\"),\n",
        "    \"t.user_session = s.user_session AND t.event_time = s.event_time\"\n",
        ").whenMatchedUpdateAll() \\\n",
        " .whenNotMatchedInsertAll() \\\n",
        " .execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Query Historical Versions (Time Travel)\n",
        "\n",
        "Delta Lake supports reading older versions of a table using:\n",
        "- versionAsOf (specific version number)\n",
        "- timestampAsOf (data as of a date/time)"
      ],
      "metadata": {
        "id": "H-O2t3FGhnun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read older version of Delta table\n",
        "v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/delta/events\")\n",
        "\n",
        "# Read Delta table as of a timestamp\n",
        "yesterday = spark.read.format(\"delta\") \\\n",
        "    .option(\"timestampAsOf\", \"2024-01-01\") \\\n",
        "    .load(\"/delta/events\")"
      ],
      "metadata": {
        "id": "Q8oahjf5hrus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Optimize Tables\n",
        "\n",
        "OPTIMIZE compacts small files into bigger ones to improve performance.\n",
        "ZORDER improves query speed when filtering on specific columns."
      ],
      "metadata": {
        "id": "ZcI-n0-Qh0bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"OPTIMIZE events_table ZORDER BY (event_type, user_id)\")"
      ],
      "metadata": {
        "id": "F65iAXL3h1v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Clean Old Files (VACUUM)\n",
        "\n",
        "VACUUM removes unused old files to save storage.\n",
        "Retention period is set to avoid deleting important historical versions too soon."
      ],
      "metadata": {
        "id": "-ex8-qmMh_cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"VACUUM events_table RETAIN 168 HOURS\")"
      ],
      "metadata": {
        "id": "HesWbXXkiASQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaway\n",
        "\n",
        "Delta Lake supports reliable data pipelines by enabling:\n",
        "- Incremental upserts (MERGE)\n",
        "- Auditing and rollback (Time Travel)\n",
        "- Performance improvements (OPTIMIZE + ZORDER)\n",
        "- Storage cleanup (VACUUM)"
      ],
      "metadata": {
        "id": "1_FtIbWoiKcD"
      }
    }
  ]
}