{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCm_AbAA1QI2",
        "outputId": "edf4505d-7c6c-4f42-85f9-6cf3b43d2a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Started\n",
            "Created temp view: silver_events\n",
            "Rows: 200000\n",
            "\n",
            " 1) EXPLAIN Query Plan\n",
            "== Parsed Logical Plan ==\n",
            "'Project [*]\n",
            "+- 'Filter ('event_type = purchase)\n",
            "   +- 'UnresolvedRelation [silver_events], [], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: bigint, product_id: bigint, event_type: string, event_date: bigint, price: double\n",
            "Project [user_id#2L, product_id#3L, event_type#4, event_date#5L, price#6]\n",
            "+- Filter (event_type#4 = purchase)\n",
            "   +- SubqueryAlias silver_events\n",
            "      +- View (`silver_events`, [user_id#2L, product_id#3L, event_type#4, event_date#5L, price#6])\n",
            "         +- Project [(row_id#1L % cast(50000 as bigint)) AS user_id#2L, (row_id#1L % cast(2000 as bigint)) AS product_id#3L, CASE WHEN ((row_id#1L % cast(3 as bigint)) = cast(0 as bigint)) THEN purchase WHEN ((row_id#1L % cast(3 as bigint)) = cast(1 as bigint)) THEN view ELSE cart END AS event_type#4, ((row_id#1L % cast(30 as bigint)) + cast(1 as bigint)) AS event_date#5L, (rand(5601479030039924967) * cast(500 as double)) AS price#6]\n",
            "            +- Project [id#0L AS row_id#1L]\n",
            "               +- Range (0, 200000, step=1, splits=Some(2))\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Filter (event_type#4 = purchase)\n",
            "+- Project [(id#0L % 50000) AS user_id#2L, (id#0L % 2000) AS product_id#3L, CASE WHEN ((id#0L % 3) = 0) THEN purchase WHEN ((id#0L % 3) = 1) THEN view ELSE cart END AS event_type#4, ((id#0L % 30) + 1) AS event_date#5L, (rand(5601479030039924967) * 500.0) AS price#6]\n",
            "   +- Range (0, 200000, step=1, splits=Some(2))\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Filter (event_type#4 = purchase)\n",
            "+- *(1) Project [(id#0L % 50000) AS user_id#2L, (id#0L % 2000) AS product_id#3L, CASE WHEN ((id#0L % 3) = 0) THEN purchase WHEN ((id#0L % 3) = 1) THEN view ELSE cart END AS event_type#4, ((id#0L % 30) + 1) AS event_date#5L, (rand(5601479030039924967) * 500.0) AS price#6]\n",
            "   +- *(1) Range (0, 200000, step=1, splits=2)\n",
            "\n",
            "\n",
            " 2) Writing partitioned table (parquet) to simulate partition pruning...\n",
            " Partitioned table created & loaded: silver_events_part\n",
            "\n",
            " 3) Benchmark (non-partitioned vs partitioned)\n",
            " Non-partitioned filter | rows=6667 | time=0.525s\n",
            " Partitioned filter (pruning expected) | rows=6667 | time=0.840s\n",
            "\n",
            " 4) Cache table for iterative queries\n",
            " After cache: user_id filter | rows=4 | time=0.703s\n",
            " After cache (repeat): should be faster | rows=4 | time=0.703s\n",
            "\n",
            " Task 10 Completed Successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Task 10: Spark Query Optimization (Colab)\n",
        "# Explain Plan + Partitioning + Benchmark + Cache\n",
        "# ==========================================\n",
        "\n",
        "!pip -q install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, rand, when\n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Task10-Optimization\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Started\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 1) Create a demo silver.events dataset\n",
        "# ------------------------------------------\n",
        "n = 2_000_00  # adjust if Colab becomes slow (try 200k, 500k, 1M)\n",
        "\n",
        "df = spark.range(0, n).withColumnRenamed(\"id\", \"row_id\")\n",
        "\n",
        "events = df.select(\n",
        "    (col(\"row_id\") % 50000).alias(\"user_id\"),\n",
        "    (col(\"row_id\") % 2000).alias(\"product_id\"),\n",
        "    when((col(\"row_id\") % 3) == 0, \"purchase\")\n",
        "    .when((col(\"row_id\") % 3) == 1, \"view\")\n",
        "    .otherwise(\"cart\").alias(\"event_type\"),\n",
        "    (col(\"row_id\") % 30 + 1).alias(\"event_date\"),\n",
        "    (rand() * 500).alias(\"price\")\n",
        ")\n",
        "\n",
        "events.createOrReplaceTempView(\"silver_events\")\n",
        "\n",
        "print(\"Created temp view: silver_events\")\n",
        "print(\"Rows:\", events.count())\n",
        "\n",
        "# ------------------------------------------\n",
        "# 2) Explain Query Plan\n",
        "# ------------------------------------------\n",
        "print(\"\\n 1) EXPLAIN Query Plan\")\n",
        "spark.sql(\"SELECT * FROM silver_events WHERE event_type = 'purchase'\").explain(True)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3) Partitioned Table (Parquet partitioning simulation)\n",
        "# In Databricks: PARTITIONED BY (event_date, event_type)\n",
        "# Here: write as partitioned parquet to disk\n",
        "# ------------------------------------------\n",
        "output_path = \"/content/silver_events_part\"\n",
        "\n",
        "print(\"\\n 2) Writing partitioned table (parquet) to simulate partition pruning...\")\n",
        "events.write.mode(\"overwrite\").partitionBy(\"event_date\", \"event_type\").parquet(output_path)\n",
        "\n",
        "events_part = spark.read.parquet(output_path)\n",
        "events_part.createOrReplaceTempView(\"silver_events_part\")\n",
        "\n",
        "print(\" Partitioned table created & loaded: silver_events_part\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4) Benchmark: query on non-partitioned vs partitioned\n",
        "# ------------------------------------------\n",
        "def benchmark(query, label):\n",
        "    start = time.time()\n",
        "    cnt = spark.sql(query).count()\n",
        "    end = time.time()\n",
        "    print(f\" {label} | rows = {cnt} | time = {end-start:.3f}s\")\n",
        "\n",
        "print(\"\\n 3) Benchmark (non-partitioned vs partitioned)\")\n",
        "\n",
        "benchmark(\n",
        "    \"SELECT * FROM silver_events WHERE event_type='purchase' AND event_date = 10\",\n",
        "    \"Non-partitioned filter\"\n",
        ")\n",
        "\n",
        "benchmark(\n",
        "    \"SELECT * FROM silver_events_part WHERE event_type='purchase' AND event_date = 10\",\n",
        "    \"Partitioned filter (pruning expected)\"\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 5) Cache for iterative queries\n",
        "# ------------------------------------------\n",
        "print(\"\\n 4) Cache table for iterative queries\")\n",
        "events_part.cache()          # Cache dataframe\n",
        "events_part.count()          # Materialize cache\n",
        "\n",
        "benchmark(\n",
        "    \"SELECT * FROM silver_events_part WHERE user_id=12345\",\n",
        "    \"After cache: user_id filter\"\n",
        ")\n",
        "\n",
        "benchmark(\n",
        "    \"SELECT * FROM silver_events_part WHERE user_id=12345\",\n",
        "    \"After cache (repeat): should be faster\"\n",
        ")\n",
        "\n",
        "print(\"\\n Task 10 Completed Successfully!\")"
      ]
    }
  ]
}