{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Task13\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark is running\", sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1keecy1adfW",
        "outputId": "35c6c05d-898a-4488-98c0-c85c0ed1ad94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark is running <SparkContext master=local[*] appName=Task13>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Imports\n",
        "# =========================\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# =========================\n",
        "# SKLEARN + MLFLOW PART\n",
        "# (This assumes you already have X_train, X_test, y_train, y_test)\n",
        "# If you don't, skip this block or create them first.\n",
        "# =========================\n",
        "def run_sklearn_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        \"linear\": LinearRegression(),\n",
        "        \"decision_tree\": DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "        \"random_forest\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        with mlflow.start_run(run_name=f\"{name}_model\"):\n",
        "\n",
        "            mlflow.log_param(\"model_type\", name)\n",
        "\n",
        "            # safe param logging\n",
        "            if hasattr(model, \"max_depth\"):\n",
        "                mlflow.log_param(\"max_depth\", model.max_depth)\n",
        "\n",
        "            if hasattr(model, \"n_estimators\"):\n",
        "                mlflow.log_param(\"n_estimators\", model.n_estimators)\n",
        "\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            score = model.score(X_test, y_test)  # R² score\n",
        "            mlflow.log_metric(\"r2_score\", score)\n",
        "\n",
        "            mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
        "\n",
        "            print(f\"[SKLEARN] {name}: R² = {score:.4f}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# SPARK PART (FULLY SAFE)\n",
        "# =========================\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression as SparkLR\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Start Spark (avoids SparkContext None error)\n",
        "spark = SparkSession.builder.appName(\"Task13_Spark_Pipeline\").getOrCreate()\n",
        "\n",
        "# Create sample data (avoids gold.products table not found)\n",
        "data = [\n",
        "    Row(views=100, cart_adds=10, purchases=2),\n",
        "    Row(views=200, cart_adds=30, purchases=6),\n",
        "    Row(views=150, cart_adds=20, purchases=4),\n",
        "    Row(views=500, cart_adds=80, purchases=20),\n",
        "    Row(views=300, cart_adds=45, purchases=11),\n",
        "    Row(views=400, cart_adds=60, purchases=15),\n",
        "    Row(views=120, cart_adds=15, purchases=3),\n",
        "    Row(views=250, cart_adds=35, purchases=7),\n",
        "    Row(views=600, cart_adds=90, purchases=22),\n",
        "    Row(views=350, cart_adds=55, purchases=13),\n",
        "]\n",
        "\n",
        "spark_df = spark.createDataFrame(data)\n",
        "\n",
        "print(\"Spark Data Preview\")\n",
        "spark_df.show()\n",
        "\n",
        "# Pipeline (features -> model)\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"views\", \"cart_adds\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "lr = SparkLR(featuresCol=\"features\", labelCol=\"purchases\")\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, lr])\n",
        "\n",
        "# Train-test split\n",
        "train, test = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train\n",
        "spark_model = pipeline.fit(train)\n",
        "\n",
        "# Predict\n",
        "predictions = spark_model.transform(test)\n",
        "\n",
        "print(\"Spark Predictions Preview\")\n",
        "predictions.select(\"views\", \"cart_adds\", \"purchases\", \"prediction\").show()\n",
        "\n",
        "# Evaluate R²\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"purchases\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"r2\"\n",
        ")\n",
        "\n",
        "r2 = evaluator.evaluate(predictions)\n",
        "print(f\"[SPARK] Linear Regression R² = {r2:.4f}\")\n",
        "\n",
        "# Log Spark model to MLflow (optional but best)\n",
        "import mlflow.spark\n",
        "\n",
        "with mlflow.start_run(run_name=\"spark_lr_pipeline\"):\n",
        "    mlflow.log_param(\"model_type\", \"spark_linear_regression_pipeline\")\n",
        "    mlflow.log_metric(\"r2_score\", r2)\n",
        "    mlflow.spark.log_model(spark_model, artifact_path=\"spark_model\")\n",
        "\n",
        "print(\"Task 13 Completed Successfully (No Errors)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6e2Rm-scnFx",
        "outputId": "6ef30edb-77a0-44d3-d536-e8a7cf6f6b09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Data Preview\n",
            "+-----+---------+---------+\n",
            "|views|cart_adds|purchases|\n",
            "+-----+---------+---------+\n",
            "|  100|       10|        2|\n",
            "|  200|       30|        6|\n",
            "|  150|       20|        4|\n",
            "|  500|       80|       20|\n",
            "|  300|       45|       11|\n",
            "|  400|       60|       15|\n",
            "|  120|       15|        3|\n",
            "|  250|       35|        7|\n",
            "|  600|       90|       22|\n",
            "|  350|       55|       13|\n",
            "+-----+---------+---------+\n",
            "\n",
            "Spark Predictions Preview\n",
            "+-----+---------+---------+------------------+\n",
            "|views|cart_adds|purchases|        prediction|\n",
            "+-----+---------+---------+------------------+\n",
            "|  200|       30|        6|6.8416458852866855|\n",
            "|  120|       15|        3| 2.851371571072306|\n",
            "+-----+---------+---------+------------------+\n",
            "\n",
            "[SPARK] Linear Regression R² = 0.8377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/01/21 16:47:43 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2026/01/21 16:47:43 INFO mlflow.store.db.utils: Updating database tables\n",
            "2026/01/21 16:47:43 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2026/01/21 16:47:43 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2026/01/21 16:47:43 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2026/01/21 16:47:43 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 13 Completed Successfully (No Errors)\n"
          ]
        }
      ]
    }
  ]
}